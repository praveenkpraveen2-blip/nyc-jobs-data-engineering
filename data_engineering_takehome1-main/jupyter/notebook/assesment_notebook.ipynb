{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = SparkSession.builder. \\\n\u001b[32m      4\u001b[39m     appName(\u001b[33m\"\u001b[39m\u001b[33mpyspark-1\u001b[39m\u001b[33m\"\u001b[39m). \\\n\u001b[32m      5\u001b[39m     getOrCreate()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "from pyspark.sql.functions import current_date, add_months\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-1\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job ID: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: string (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: string (nullable = true)\n",
      " |-- Salary Range To: string (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.count()\n\u001b[32m      2\u001b[39m df.select(\u001b[33m\"\u001b[39m\u001b[33mJob ID\u001b[39m\u001b[33m\"\u001b[39m).distinct().count()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "df.select(\"Job ID\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null % Per Column\n",
    "# -----------------------------------------------------------\n",
    "# Function: get_null_percentages\n",
    "# Calculates percentage of null values for each column\n",
    "# Helps identify columns needing cleaning or removal\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total = df.count()\n",
    "\n",
    "null_df = df.select([\n",
    "    (count(when(col(c).isNull(), c)) / total * 100).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "null_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Salary Columns\n",
    "# -----------------------------------------------------------\n",
    "# Function: clean_salary\n",
    "# Removes special characters from salary columns\n",
    "# Converts them to numeric (double) for aggregation\n",
    "# Necessary before performing statistical analysis\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def clean_salary(df):\n",
    "    return df.withColumn(\n",
    "        \"salary_from\",\n",
    "        regexp_replace(col(\"Salary Range From\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    "    ).withColumn(\n",
    "        \"salary_to\",\n",
    "        regexp_replace(col(\"Salary Range To\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date conversion\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Function: convert_dates\n",
    "# Converts posting date string into Spark date type\n",
    "# Enables time-based filtering and analysis\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def convert_dates(df):\n",
    "    return df.withColumn(\n",
    "        \"posting_date\",\n",
    "        to_date(col(\"Posting Date\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Average Salary\n",
    "# -----------------------------------------------------------\n",
    "# Function: add_avg_salary\n",
    "# Creates a derived column representing midpoint salary\n",
    "# Used for more realistic salary analytics\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_salary(df):\n",
    "    return df.withColumn(\n",
    "        \"avg_salary\",\n",
    "        (col(\"salary_from\") + col(\"salary_to\")) / 2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Encoding\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Function: encode_degree\n",
    "# Encodes education level into numeric scale:\n",
    "# 0 = No degree mentioned\n",
    "# 1 = Bachelor\n",
    "# 2 = Master\n",
    "# 3 = PhD\n",
    "# Enables correlation analysis with salary\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode_degree(df):\n",
    "    return df.withColumn(\n",
    "        \"degree_encoded\",\n",
    "        when(lower(col(\"Minimum Qual Requirements\")).contains(\"phd\"), 3)\n",
    "        .when(lower(col(\"Minimum Qual Requirements\")).contains(\"master\"), 2)\n",
    "        .when(lower(col(\"Minimum Qual Requirements\")).contains(\"bachelor\"), 1)\n",
    "        .otherwise(0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Skills\n",
    "# -----------------------------------------------------------\n",
    "# Function: extract_skills\n",
    "# Creates binary flags for selected technical skills\n",
    "# Skill presence is detected via keyword matching in job description\n",
    "# Enables salary comparison by skill demand\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(df):\n",
    "    skills = [\"python\", \"sql\", \"aws\", \"spark\", \"azure\", \"hadoop\"]\n",
    "    for skill in skills:\n",
    "        df = df.withColumn(\n",
    "            f\"skill_{skill}\",\n",
    "            when(lower(col(\"Job Description\")).contains(skill), 1).otherwise(0)\n",
    "        )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Salary Range From\", \"Salary Range To\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 1 CALCULATIONS Top 10 Categories\n",
    "# -----------------------------------------------------------\n",
    "# KPI 1: Top 10 Job Categories\n",
    "# Groups dataset by job category and counts postings\n",
    "# Sorted in descending order to identify highest demand categories\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = (\n",
    "    df.groupBy(\"Job Category\")\n",
    "      .count()\n",
    "      .orderBy(col(\"count\").desc())\n",
    "      .limit(10)\n",
    ")\n",
    "\n",
    "top_10_pd = top_10.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=top_10_pd,\n",
    "    x=\"count\",\n",
    "    y=\"Job Category\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 10 Job Categories by Number of Postings\")\n",
    "plt.xlabel(\"Number of Postings\")\n",
    "plt.ylabel(\"Job Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 2 – Salary Distribution per Category\n",
    "# -----------------------------------------------------------\n",
    "# Purpose:\n",
    "# Analyze how salary varies across different job categories.\n",
    "# Compute statistical metrics including:\n",
    "#   - Mean salary\n",
    "#   - Minimum salary\n",
    "#   - Maximum salary\n",
    "#   - Standard deviation\n",
    "#\n",
    "# Uses the engineered \"avg_salary\" feature for consistency.\n",
    "# Spark aggregation ensures scalability on large datasets.\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = top_10_pd[\"Job Category\"].tolist()\n",
    "\n",
    "salary_sample = df.filter(\n",
    "    col(\"Job Category\").isin(top_categories)\n",
    ").select(\"Job Category\", \"avg_salary\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(\n",
    "    data=salary_sample,\n",
    "    x=\"Job Category\",\n",
    "    y=\"avg_salary\"\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Salary Distribution per Top Job Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 3 – Correlation Degree vs Salary\n",
    "# -----------------------------------------------------------\n",
    "# Purpose:\n",
    "# Determine whether higher educational qualifications\n",
    "# are associated with higher salary ranges.\n",
    "#\n",
    "# Degree encoding scale:\n",
    "#   0 = No degree mentioned\n",
    "#   1 = Bachelor\n",
    "#   2 = Master\n",
    "#   3 = PhD\n",
    "#\n",
    "# Uses Spark's built-in correlation method for distributed computation.\n",
    "# Result interpretation:\n",
    "#   > 0  → Positive relationship\n",
    "#   = 0  → No linear relationship\n",
    "#   < 0  → Negative relationship\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_salary = (\n",
    "    df.groupBy(\"degree_encoded\")\n",
    "      .agg(avg(\"avg_salary\").alias(\"mean_salary\"))\n",
    ")\n",
    "\n",
    "degree_salary_pd = degree_salary.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    data=degree_salary_pd,\n",
    "    x=\"degree_encoded\",\n",
    "    y=\"mean_salary\"\n",
    ")\n",
    "\n",
    "plt.title(\"Average Salary by Degree Level\")\n",
    "plt.xlabel(\"Degree Level (0=None, 1=Bachelor, 2=Master, 3=PhD)\")\n",
    "plt.ylabel(\"Average Salary\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 4 – Highest Salary per Agency\n",
    "# -----------------------------------------------------------\n",
    "# KPI 4: Highest Salary per Agency\n",
    "# Uses window function to rank salaries within each agency\n",
    "# Selects highest paying job per agency\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = Window.partitionBy(\"Agency\").orderBy(col(\"salary_to\").desc())\n",
    "\n",
    "highest_salary = (\n",
    "    df.withColumn(\"rank\", row_number().over(window))\n",
    "      .filter(col(\"rank\") == 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_highest = highest_salary_df.orderBy(desc(\"salary_to\")).limit(10)\n",
    "top_10_pd = top_10_highest.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=top_10_pd,\n",
    "    x=\"salary_to\",\n",
    "    y=\"Agency\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 10 Highest Paying Job Postings per Agency\")\n",
    "plt.xlabel(\"Maximum Salary\")\n",
    "plt.ylabel(\"Agency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 5 – Avg Salary per Agency (Last 2 Years)\n",
    "# -----------------------------------------------------------\n",
    "# KPI 5: Average Salary per Agency (Last 2 Years)\n",
    "# Filters dataset dynamically using current date\n",
    "# Aggregates average salary per agency\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_2_years = df.filter(\n",
    "    col(\"posting_date\") >= add_months(current_date(), -24)\n",
    ")\n",
    "\n",
    "agency_avg = (\n",
    "    last_2_years.groupBy(\"Agency\")\n",
    "                .agg(avg(\"avg_salary\").alias(\"avg_salary_last_2_years\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_agency_avg = agency_avg_salary_df.limit(10)\n",
    "agency_avg_pd = top_10_agency_avg.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=agency_avg_pd,\n",
    "    x=\"avg_salary_last_2_years\",\n",
    "    y=\"Agency\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 10 Agencies by Avg Salary (Last 2 Years)\")\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Agency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPI 6 – Highest Paid Skills\n",
    "# -----------------------------------------------------------\n",
    "# KPI 6: Highest Paid Skills\n",
    "# Calculates average salary for postings containing each skill\n",
    "# Results used for skill ranking visualization\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_salary = []\n",
    "\n",
    "for skill in [\"python\",\"sql\",\"aws\",\"spark\",\"azure\"]:\n",
    "    avg_val = df.filter(col(f\"skill_{skill}\") == 1) \\\n",
    "                .agg(avg(\"avg_salary\")) \\\n",
    "                .collect()[0][0]\n",
    "    skill_salary.append((skill, avg_val))\n",
    "\n",
    "spark.createDataFrame(skill_salary, [\"Skill\",\"Avg Salary\"]) \\\n",
    "     .orderBy(col(\"Avg Salary\").desc()) \\\n",
    "     .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_salary_pd = skill_salary_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=skill_salary_pd,\n",
    "    x=\"Avg Salary\",\n",
    "    y=\"Skill\"\n",
    ")\n",
    "\n",
    "plt.title(\"Highest Paid Skills in NYC Job Market\")\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Skill\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_negative_salary(df):\n",
    "    assert df.filter(col(\"avg_salary\") < 0).count() == 0\n",
    "\n",
    "def test_salary_columns_numeric(df):\n",
    "    assert dict(df.dtypes)[\"salary_from\"] == \"double\"\n",
    "\n",
    "def test_no_duplicate_job_ids(df):\n",
    "    total = df.count()\n",
    "    distinct = df.select(\"Job ID\").distinct().count()\n",
    "    assert total == distinct\n",
    "\n",
    "def test_degree_encoded_exists(df):\n",
    "    assert \"degree_encoded\" in df.columns\n",
    "\n",
    "def test_date_conversion(df):\n",
    "    assert dict(df.dtypes)[\"posting_date\"] == \"date\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
